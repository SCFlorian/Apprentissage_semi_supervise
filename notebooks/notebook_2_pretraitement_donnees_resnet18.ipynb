{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "084b0e63",
   "metadata": {},
   "source": [
    "# Mission - Analysez des images médicales avec des méthodes semi-supervisées\n",
    "\n",
    "Vous êtes Data Scientist junior spécialisé en Computer Vision au sein de CurelyticsIA, une startup innovante dans le domaine de la e-santé. L’entreprise développe des solutions basées sur l’intelligence artificielle pour assister les professionnels de santé dans l’analyse d’images médicales, en particulier des IRM.\n",
    " \n",
    "Dans le cadre d’un nouveau projet R&D, CurelyticsIA souhaite explorer la possibilité d’automatiser la détection de tumeurs du cerveau. Un ensemble conséquent de radios a été collecté : la majorité de ces images ne dispose d’aucun étiquetage, tandis qu’un sous-ensemble limité a été annoté par des radiologues experts.\n",
    " \n",
    "Vous êtes chargé de concevoir une première exploration analytique du jeu de données. Plus précisément, votre mission est de :\n",
    "- Explorer les images et extraire des caractéristiques visuelles via un modèle pré-entraîné ;\n",
    "- Appliquer des méthodes de clustering pour identifier des structures ou regroupements dans les données ;\n",
    "- Mettre en œuvre une méthode d’apprentissage semi-supervisé à partir des quelques étiquettes disponibles ;\n",
    "- Synthétiser vos résultats, formuler des recommandations, et les présenter à votre équipe projet.\n",
    "\n",
    "**Mail à prendre en compte :**\n",
    "\n",
    "Comme discuté lors de notre dernière réunion, tu es assigné à la première phase du projet BrainScanAI. Tu trouveras en pièce jointe un fichier zip contenant :\n",
    "- Le jeu de données de radiographies (en format PNG + métadonnées anonymisées),\n",
    "- Une documentation technique sur le format des images ;\n",
    "- Une liste restreinte de labels annotés par nos partenaires hospitaliers (normal/cancéreux). \n",
    "\n",
    "Pour info, notre budget actuel pour la labellisation par IA est de 300 euros pour ce dataset. \n",
    "\n",
    "Tes objectifs :\n",
    "1) Extraire des caractéristiques visuelles pertinentes à l’aide d’un modèle pré-entraîné (type ResNet ou équivalent).\n",
    "2) Réaliser un clustering exploratoire pour identifier des regroupements naturels.\n",
    "3) Mettre en œuvre une méthode semi-supervisée en exploitant les labels partiels pour prédire les étiquettes manquantes.\n",
    "4) Proposer des livrables au format Notebook contenant :\n",
    "    - l’extraction des features\n",
    "    - le preprocessing adapté au(x) modèle(s) utilisés\n",
    "    - l’analyse non-supervisée (.ipynb)\n",
    "    - l’entraînement de modèles de clustering\n",
    "    - l’approche semi-supervisée (.ipynb)\n",
    " \n",
    "Ces livrables doivent être accompagnés d’un support de présentation proposant des recommandations techniques pour un passage à l’échelle (budget de 5 000 euros pour 4 millions d’images à labelliser). Est-ce que ce passage te paraît faisable et si oui, sous quelles conditions ?\n",
    "\n",
    "5) Rédiger une synthèse de ton approche et de tes résultats dans un support de présentation. Les contraintes :\n",
    "    - Travailler en Python.\n",
    "    - Tester plusieurs algorithmes.\n",
    "    - Avoir des métriques pertinentes en fonction de l’erreur la plus importante (F1, Acc, Précision, ou autre ?).\n",
    "    - Clairement définir ce que tu considères comme un objectif atteint (“definition of done”).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dfa403",
   "metadata": {},
   "source": [
    "# Étape 2 - Prétraitez et extrayez les features\n",
    "Préparez les images (redimensionnement, normalisation) et utilisez un modèle pré-entraîné (ex : ResNet) pour extraire des embeddings visuels.\n",
    " \n",
    "**Prérequis**\n",
    "- Avoir nettoyé et formaté les données image.\n",
    "- Avoir compris le fonctionnement des CNNs.\n",
    "\n",
    "**Résultat attendu** \n",
    "- Vecteurs de features pour chaque image, sauvegardés dans un tableau exploitable.\n",
    "\n",
    "**Recommandations**\n",
    "- Geler les couches convolutionnelles.\n",
    "- Évaluer plusieurs couches d’extraction si besoin.\n",
    "\n",
    "**Outils**\n",
    "- Torchvision\n",
    "- TensorFlow\n",
    "- Transforms\n",
    "- Numpy \n",
    "- Pandas \n",
    "- Matplotlib\n",
    "- Opencv-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa3d86",
   "metadata": {},
   "source": [
    "## Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be93f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies de base\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Librairies spécifiques\n",
    "import os # permet de travailler avec le système de fichiers\n",
    "from PIL import Image # ouvrir et manipuler des images\n",
    "\n",
    "# Librairies PyTorch\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms # pour effectuer les changements de format, entre autres\n",
    "from torchvision.models import resnet18, ResNet18_Weights # chargement du modèle ResNet 50\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # pour l'application de la couche layer3 du ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3556a3f8",
   "metadata": {},
   "source": [
    "### Pour extraire les embeddings de nos images, nous devons faire quelques transformations sur celles-ci afin de respecter la documentation de ResNet (modèle pré-entaîné utilisé ici). Nous avons suivi cette dernière pour transformer nos images médicales :\n",
    "- https://pytorch.org/hub/pytorch_vision_resnet/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e80e905",
   "metadata": {},
   "source": [
    "#### Preprocess afin :\n",
    "* de réduire la taille\n",
    "* de les transformer en tensor\n",
    "* de les normaliser \n",
    "* pour respecter les attentes du ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22d83b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256), # car entraînement ImageNet sur la taille 256\n",
    "    transforms.CenterCrop(224), # ResNet a été entraîné sur du 224x224\n",
    "    transforms.ToTensor(), # transformation de l'image PIL en tensor PyTorch\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # normalisation du RGB selon les stats ResNet\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac847c6a",
   "metadata": {},
   "source": [
    "#### Choix du modèle pré-entrâiné - ResNet18 avec application de l'avant dernière couche\n",
    "* Les filtres des convolutions, les biais, etc. sont déjà appris.\n",
    "* Le modèle sait déjà extraire des caractéristiques visuelles utiles (bords, textures, formes, objets).\n",
    "* On utilise **model.fc = nn.Identity()** afin d'utiliser l'avant dernière couche pour ne pas générer la classification mais avoir uniquement les embeddings par image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6363099b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1) #18 fait référence au nombre de couches\n",
    "model.fc = nn.Identity()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf7e977",
   "metadata": {},
   "source": [
    "#### Fonction pour appliquer le preprocess/le model et l'enregistrement des vecteurs dans une liste à l'ensemble des dossiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17caa791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(folder_path):\n",
    "    \"\"\" Génération des embeddings ET des filenames pour garder l'ordre exact \"\"\"\n",
    "    \n",
    "    embeddings = []     # liste de vecteurs\n",
    "    filenames = []      # liste des noms des fichiers dans le même ordre\n",
    "    total = 0\n",
    "\n",
    "    for img_name in sorted(os.listdir(folder_path)):  # sorted = ordre stable\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        input_tensor = preprocess(img)\n",
    "        input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            emb = model(input_batch)\n",
    "            emb = emb.squeeze(0)\n",
    "        \n",
    "        embeddings.append(emb.numpy())\n",
    "        filenames.append(img_name)\n",
    "        total += 1\n",
    "\n",
    "    print(f\"Nombre d'images : {total}\")\n",
    "    print(f\"Shape embedding : {embeddings[0].shape}\")\n",
    "\n",
    "    return embeddings, filenames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0916f74",
   "metadata": {},
   "source": [
    "#### Application sur le dossier normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caa7b153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'images : 50\n",
      "Shape embedding : (512,)\n"
     ]
    }
   ],
   "source": [
    "embeddings_normal, filenames_normal = extract_embeddings(\"../mri_dataset_brain_cancer_oc/avec_labels/normal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07063253",
   "metadata": {},
   "source": [
    "* On retrouve nos 50 images mais avec nos 2048 caractéristiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae26c7ba",
   "metadata": {},
   "source": [
    "#### Application sur le dossier cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d67680b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'images : 50\n",
      "Shape embedding : (512,)\n"
     ]
    }
   ],
   "source": [
    "embeddings_cancer, filenames_cancer  = extract_embeddings(\"../mri_dataset_brain_cancer_oc/avec_labels/cancer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3264772",
   "metadata": {},
   "source": [
    "* On retrouve nos 50 images mais avec nos 2048 caractéristiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bf24e7",
   "metadata": {},
   "source": [
    "#### Application sur le dossier sans label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "291622fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'images : 1406\n",
      "Shape embedding : (512,)\n"
     ]
    }
   ],
   "source": [
    "embeddings_sans_label, filenames_sans_label = extract_embeddings(\"../mri_dataset_brain_cancer_oc/sans_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e0c8a",
   "metadata": {},
   "source": [
    "* On retrouve nos 1406 images mais avec nos 2048 caractéristiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531ba67b",
   "metadata": {},
   "source": [
    "### Enregistrement en DataFrame, on constitue nos jeux de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bb1b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabeled = pd.DataFrame(embeddings_sans_label)\n",
    "df_unlabeled[\"filename\"] = filenames_sans_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dacbf78",
   "metadata": {},
   "source": [
    "#### On identifie les labels : 0 pour normal et 1 pour cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbb631dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal = pd.DataFrame(embeddings_normal)\n",
    "df_normal[\"label\"] = 0\n",
    "df_normal[\"filename\"] = filenames_normal\n",
    "\n",
    "df_cancer = pd.DataFrame(embeddings_cancer)\n",
    "df_cancer[\"label\"] = 1\n",
    "df_cancer[\"filename\"] = filenames_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e19d8",
   "metadata": {},
   "source": [
    "#### On rassemble nos 2 jeux labellisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d61760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled = pd.concat([df_normal, df_cancer], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2aedb",
   "metadata": {},
   "source": [
    "#### Sauvegarde des DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51326664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled.to_csv(\"../data/processed/df_labeled_resnet18.csv\", index=False)\n",
    "df_unlabeled.to_csv(\"../data/processed/df_unlabeled_resnet18.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c8c363",
   "metadata": {},
   "source": [
    "### À des fins de comparaison, on va utiliser de nouveau le modèle pré-entrâiné - ResNet50 avec application de la couche layer3 (res4)\n",
    "* Objectif : récupérer une couche moins abstraite que l'avant dernière couche.\n",
    "* **Couches basses (près de l’entrée)** : features “peu abstraites”. Elles capturent des choses simples et locales :\n",
    "    - bords\n",
    "    - coins\n",
    "    - textures fines\n",
    "    - motifs locaux\n",
    "    - Exemple : détecter \"une petite zone lumineuse\".\n",
    "* Layer3 se trouve dans cette zone -> features locales, moins conceptuelles.\n",
    "\n",
    "* **Couches hautes (layer4 / fc)** : features “abstraites”. Elles capturent :\n",
    "    - la forme globale\n",
    "    - les parties de l’objet\n",
    "    - la structure anatomique\n",
    "    - des concepts visuels complexes\n",
    "* On les appelle “abstraites” car elles sont loin des pixels et proches du concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d68cb",
   "metadata": {},
   "source": [
    "#### On commence par mettre en place le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "791d53f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1) # Charge ResNet50 pré-entraîné sur ImageNet\n",
    "model = model.to(device) # Envoie le modèle sur GPU si disponible, sinon CPU\n",
    "model.eval() # Met le modèle en mode évaluation (désactive dropout, batchnorm training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d878470f",
   "metadata": {},
   "source": [
    "#### Fonction pour récupérer les caractéristiques de la couche layer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a365ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer3_output = None # Variable globale où sera stockée la sortie de layer3\n",
    "\n",
    "def hook(module, input, output): # Son rôle : stocker les features de layer3 avant la suite du réseau\n",
    "    global layer3_output # On dit que layer3_output est une variable globale\n",
    "    layer3_output = output # Le hook copie la sortie de layer3 dans layer3_output\n",
    "\n",
    "# On attache le hook\n",
    "handle = model.layer3.register_forward_hook(hook) # handle est un objet qui représente le hook en mémoire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc8cd3a",
   "metadata": {},
   "source": [
    "#### Transformation pour être utilisé par la suite dans nos analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "370ed6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap = nn.AdaptiveAvgPool2d((1, 1)) # Pooling global pour transformer (C,H,W) -> (C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebce28c",
   "metadata": {},
   "source": [
    "#### Fonction pour appliquer le preprocess/le model et l'enregistrement des vecteurs dans une liste à l'ensemble des dossiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39116ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_layer3_features(model, folder, preprocess):\n",
    "    \"\"\"\n",
    "    Extrait les features de la couche layer3 de ResNet50 pour toutes les images d'un dossier.\n",
    "    \n",
    "    model : modèle ResNet18 déjà chargé + hook déjà attaché\n",
    "    folder : dossier contenant les images\n",
    "    preprocess : transformations torchvision (Resize, ToTensor, Normalize)\n",
    "    \"\"\"\n",
    "\n",
    "    features = []\n",
    "    filenames = []\n",
    "\n",
    "    # Parcours des images du dossier\n",
    "    for img_name in os.listdir(folder):\n",
    "        if not img_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(folder, img_name)\n",
    "\n",
    "        # Chargement de l'image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Prétraitement\n",
    "        x = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "        # Forward -> le hook récupère layer3_output automatiquement\n",
    "        with torch.no_grad():\n",
    "            _ = model(x)\n",
    "\n",
    "        # GAP -> vecteur 1D\n",
    "        vect = gap(layer3_output).squeeze().cpu().numpy()\n",
    "\n",
    "        # Stockage\n",
    "        features.append(vect)\n",
    "        filenames.append(img_name)\n",
    "\n",
    "    # Construction du DataFrame\n",
    "    df = pd.DataFrame(features)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a64c0d6",
   "metadata": {},
   "source": [
    "#### Application de la fonction sur nos différents jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49316083",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal_layer3 = extract_layer3_features(model, \"../mri_dataset_brain_cancer_oc/avec_labels/normal\", preprocess)\n",
    "df_cancer_layer3 = extract_layer3_features(model, \"../mri_dataset_brain_cancer_oc/avec_labels/cancer\", preprocess)\n",
    "df_unlabeled_layer3 = extract_layer3_features(model, \"../mri_dataset_brain_cancer_oc/sans_label\", preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609fa266",
   "metadata": {},
   "source": [
    "#### Création des colonnes avec les labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7338695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal_layer3[\"label\"] = 0\n",
    "df_cancer_layer3[\"label\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98550061",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled_layer3 = pd.concat([df_normal_layer3, df_cancer_layer3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19856d7",
   "metadata": {},
   "source": [
    "#### Sauvegarde des données en csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a2fccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled_layer3.to_csv(\"../data/processed/df_labeled_resnet18_layer3.csv\", index=False)\n",
    "df_unlabeled_layer3.to_csv(\"../data/processed/df_unlabeled_resnet18_layer3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe4729",
   "metadata": {},
   "source": [
    "#### On enlève le suivi du handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1db36b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle.remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apprentissage-semi-supervise-9977Hg6q-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
