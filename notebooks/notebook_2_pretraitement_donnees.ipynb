{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "084b0e63",
   "metadata": {},
   "source": [
    "# Mission - Analysez des images médicales avec des méthodes semi-supervisées\n",
    "\n",
    "Vous êtes Data Scientist junior spécialisé en Computer Vision au sein de CurelyticsIA, une startup innovante dans le domaine de la e-santé. L’entreprise développe des solutions basées sur l’intelligence artificielle pour assister les professionnels de santé dans l’analyse d’images médicales, en particulier des IRM.\n",
    " \n",
    "Dans le cadre d’un nouveau projet R&D, CurelyticsIA souhaite explorer la possibilité d’automatiser la détection de tumeurs du cerveau. Un ensemble conséquent de radios a été collecté : la majorité de ces images ne dispose d’aucun étiquetage, tandis qu’un sous-ensemble limité a été annoté par des radiologues experts.\n",
    " \n",
    "Vous êtes chargé de concevoir une première exploration analytique du jeu de données. Plus précisément, votre mission est de :\n",
    "- Explorer les images et extraire des caractéristiques visuelles via un modèle pré-entraîné ;\n",
    "- Appliquer des méthodes de clustering pour identifier des structures ou regroupements dans les données ;\n",
    "- Mettre en œuvre une méthode d’apprentissage semi-supervisé à partir des quelques étiquettes disponibles ;\n",
    "- Synthétiser vos résultats, formuler des recommandations, et les présenter à votre équipe projet.\n",
    "\n",
    "**Mail à prendre en compte :**\n",
    "\n",
    "Comme discuté lors de notre dernière réunion, tu es assigné à la première phase du projet BrainScanAI. Tu trouveras en pièce jointe un fichier zip contenant :\n",
    "- Le jeu de données de radiographies (en format PNG + métadonnées anonymisées),\n",
    "- Une documentation technique sur le format des images ;\n",
    "- Une liste restreinte de labels annotés par nos partenaires hospitaliers (normal/cancéreux). \n",
    "\n",
    "Pour info, notre budget actuel pour la labellisation par IA est de 300 euros pour ce dataset. \n",
    "\n",
    "Tes objectifs :\n",
    "1) Extraire des caractéristiques visuelles pertinentes à l’aide d’un modèle pré-entraîné (type ResNet ou équivalent).\n",
    "2) Réaliser un clustering exploratoire pour identifier des regroupements naturels.\n",
    "3) Mettre en œuvre une méthode semi-supervisée en exploitant les labels partiels pour prédire les étiquettes manquantes.\n",
    "4) Proposer des livrables au format Notebook contenant :\n",
    "    - l’extraction des features\n",
    "    - le preprocessing adapté au(x) modèle(s) utilisés\n",
    "    - l’analyse non-supervisée (.ipynb)\n",
    "    - l’entraînement de modèles de clustering\n",
    "    - l’approche semi-supervisée (.ipynb)\n",
    " \n",
    "Ces livrables doivent être accompagnés d’un support de présentation proposant des recommandations techniques pour un passage à l’échelle (budget de 5 000 euros pour 4 millions d’images à labelliser). Est-ce que ce passage te paraît faisable et si oui, sous quelles conditions ?\n",
    "\n",
    "5) Rédiger une synthèse de ton approche et de tes résultats dans un support de présentation. Les contraintes :\n",
    "    - Travailler en Python.\n",
    "    - Tester plusieurs algorithmes.\n",
    "    - Avoir des métriques pertinentes en fonction de l’erreur la plus importante (F1, Acc, Précision, ou autre ?).\n",
    "    - Clairement définir ce que tu considères comme un objectif atteint (“definition of done”).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dfa403",
   "metadata": {},
   "source": [
    "# Étape 2 - Prétraitez et extrayez les features\n",
    "Préparez les images (redimensionnement, normalisation) et utilisez un modèle pré-entraîné (ex : ResNet) pour extraire des embeddings visuels.\n",
    " \n",
    "**Prérequis**\n",
    "- Avoir nettoyé et formaté les données image.\n",
    "- Avoir compris le fonctionnement des CNNs.\n",
    "\n",
    "**Résultat attendu** \n",
    "- Vecteurs de features pour chaque image, sauvegardés dans un tableau exploitable.\n",
    "\n",
    "**Recommandations**\n",
    "- Geler les couches convolutionnelles.\n",
    "- Évaluer plusieurs couches d’extraction si besoin.\n",
    "\n",
    "**Outils**\n",
    "- Torchvision\n",
    "- TensorFlow\n",
    "- Transforms\n",
    "- Numpy \n",
    "- Pandas \n",
    "- Matplotlib\n",
    "- Opencv-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa3d86",
   "metadata": {},
   "source": [
    "## Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be93f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies de base\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Librairies spécifiques\n",
    "import os # permet de travailler avec le système de fichiers\n",
    "from PIL import Image # ouvrir et manipuler des images\n",
    "\n",
    "# Librairies PyTorch\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms # pour effectuer les changements de format, entre autres\n",
    "from torchvision.models import resnet18, ResNet18_Weights # chargement du modèle ResNet 18\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3556a3f8",
   "metadata": {},
   "source": [
    "### Pour extraire les embeddings de nos images, nous devons faire quelques transformations sur celles-ci afin de respecter la documentation de ResNet (modèle pré-entaîné utilisé ici). Nous avons suivi cette dernière pour transformer nos images médicales :\n",
    "- https://pytorch.org/hub/pytorch_vision_resnet/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e80e905",
   "metadata": {},
   "source": [
    "#### Preprocess afin :\n",
    "* de réduire la taille\n",
    "* de les transformer en tensor\n",
    "* de les normaliser \n",
    "* pour respecter les attentes du ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22d83b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256), # car entraînement ImageNet sur la taille 256\n",
    "    transforms.CenterCrop(224), # ResNet a été entraîné sur du 224x224\n",
    "    transforms.ToTensor(), # transformation de l'image PIL en tensor PyTorch\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # normalisation du RGB selon les stats ResNet\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac847c6a",
   "metadata": {},
   "source": [
    "#### Choix du modèle pré-entrâiné - ResNet18\n",
    "* Les filtres des convolutions, les biais, etc. sont déjà appris.\n",
    "* Le modèle sait déjà extraire des caractéristiques visuelles utiles (bords, textures, formes, objets).\n",
    "* On utilise **model.fc = nn.Identity()** afin d'utiliser l'avant dernière couche pour ne pas générer la classification mais avoir uniquement les embeddings par image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6363099b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet18(weights=ResNet18_Weights.DEFAULT) # 18 fait référence au nombre de couches\n",
    "model.fc = nn.Identity()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf7e977",
   "metadata": {},
   "source": [
    "#### Fonction pour appliquer le preprocess/le model et l'enregistrement des vecteurs dans une liste à l'ensemble des dossiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17caa791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(folder_path):\n",
    "    \"\"\" Génération de l'ouverture d'une image d'un dossier à l'extraction de ses features \"\"\"\n",
    "    \n",
    "    embeddings = [] # enregistrement des éléments dans une liste\n",
    "    total = 0\n",
    "\n",
    "    for img_name in os.listdir(folder_path): # chemin du dossier\n",
    "        img_path = os.path.join(folder_path, img_name) # chemin pour récupérer les images\n",
    "        img = Image.open(img_path).convert(\"RGB\") # on ouvre une image après l'autre et on force en RGB au cas où\n",
    "\n",
    "        \n",
    "        input_tensor = preprocess(img) # on applique le preprocess pour que l'image corresponde aux attentes de ResNet\n",
    "        input_batch = input_tensor.unsqueeze(0) # besoin d'un espace en 4 dimensions pour le modèle ResNet\n",
    "\n",
    "        with torch.no_grad(): # désactive le calcul du gradient\n",
    "            emb = model(input_batch) # lancement du modèle\n",
    "            emb = emb.squeeze(0)\n",
    "        embeddings.append(emb.numpy()) # enregistrement de chaque transformation avec les 512 caractéristiques\n",
    "        total +=1\n",
    "        \n",
    "    print(f\"Nombre total d’images : {total}\")\n",
    "    print(f\"Shape : {embeddings[0].shape}\")\n",
    "\n",
    "    return embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0916f74",
   "metadata": {},
   "source": [
    "#### Application sur le dossier normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caa7b153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total d’images : 50\n",
      "Shape : (512,)\n"
     ]
    }
   ],
   "source": [
    "embeddings_normal = extract_embeddings(\"../mri_dataset_brain_cancer_oc/avec_labels/normal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07063253",
   "metadata": {},
   "source": [
    "* On retrouve nos 50 images mais avec nos 512 caractéristiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae26c7ba",
   "metadata": {},
   "source": [
    "#### Application sur le dossier cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d67680b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total d’images : 50\n",
      "Shape : (512,)\n"
     ]
    }
   ],
   "source": [
    "embeddings_cancer = extract_embeddings(\"../mri_dataset_brain_cancer_oc/avec_labels/cancer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bf24e7",
   "metadata": {},
   "source": [
    "#### Application sur le dossier sans label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "291622fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total d’images : 1406\n",
      "Shape : (512,)\n"
     ]
    }
   ],
   "source": [
    "embeddings_sans_label = extract_embeddings(\"../mri_dataset_brain_cancer_oc/sans_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531ba67b",
   "metadata": {},
   "source": [
    "### Enregistrement en DataFrame, on constitue nos jeux de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afb2b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal = pd.DataFrame(np.asarray(embeddings_normal))\n",
    "df_cancer = pd.DataFrame(np.asarray(embeddings_cancer))\n",
    "df_unlabeled = pd.DataFrame(np.asarray(embeddings_sans_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dacbf78",
   "metadata": {},
   "source": [
    "#### On identifie les labels : 0 pour normal et 1 pour cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "294580ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal[\"label\"] = 0\n",
    "df_cancer[\"label\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e19d8",
   "metadata": {},
   "source": [
    "#### On rassemble nos 2 jeux labellisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d61760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled = pd.concat([df_normal, df_cancer], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2aedb",
   "metadata": {},
   "source": [
    "### Sauvegarde des DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51326664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled.to_csv(\"../data/processed/df_labeled.csv\", index=False)\n",
    "df_unlabeled.to_csv(\"../data/processed/df_unlabeled.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apprentissage-semi-supervise-9977Hg6q-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
